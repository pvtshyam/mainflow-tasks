{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609aad83-3eb7-4830-9e4c-3dd4ec190d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after feature engineering:\n",
      "   Math  Science  English  Target  Total_Score  Average_Score\n",
      "0    98       82       65       0          245      81.666667\n",
      "1    88       83       75       1          246      82.000000\n",
      "2    74       96       88       0          258      86.000000\n",
      "3    67       94       62       0          223      74.333333\n",
      "4    80       99       79       0          258      86.000000\n",
      "\n",
      "Best Hyperparameters found by GridSearchCV:\n",
      "{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation Accuracy: 0.6875\n",
      "\n",
      "Test Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate synthetic dataset\n",
    "np.random.seed(42)  # For reproducibility\n",
    "data = {\n",
    "    'Math': np.random.randint(60, 100, size=100),     # Random Math scores\n",
    "    'Science': np.random.randint(60, 100, size=100),  # Random Science scores\n",
    "    'English': np.random.randint(60, 100, size=100),  # Random English scores\n",
    "    'Target': np.random.randint(0, 2, size=100)       # Random binary target: 0 or 1 (Fail/Pass)\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Feature Engineering: Create new features\n",
    "df['Total_Score'] = df['Math'] + df['Science'] + df['English']  # Sum of subject scores\n",
    "df['Average_Score'] = df[['Math', 'Science', 'English']].mean(axis=1)  # Average of subject scores\n",
    "\n",
    "# Display the first few rows of the dataset to see the new features\n",
    "print(\"Dataset after feature engineering:\")\n",
    "print(df.head())\n",
    "\n",
    "# Split dataset into features (X) and target (y)\n",
    "X = df.drop('Target', axis=1)  # Features: all columns except 'Target'\n",
    "y = df['Target']  # Target: 'Target' column\n",
    "\n",
    "# Optionally scale the features (important for certain models like SVM)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model: Random Forest Classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Hyperparameter tuning with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],  # Number of trees in the forest\n",
    "    'max_depth': [5, 10, 15, None],  # Maximum depth of each tree\n",
    "    'min_samples_split': [2, 5],     # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],      # Minimum samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV to search over the parameter grid with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the best score\n",
    "print(\"\\nBest Hyperparameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the model with the best hyperparameters on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance: accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8549f4e-4bc9-4a3f-9772-b49151acd0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generated and saved as 'fraud_detection.csv'\n",
      "   Transaction ID      Amount    Type  Is Fraud\n",
      "0            1001  380.794718  credit         0\n",
      "1            1002  951.207163   debit         0\n",
      "2            1003  734.674002   debit         0\n",
      "3            1004  602.671899   debit         0\n",
      "4            1005  164.458454   debit         0\n",
      "\n",
      "Missing values in each column:\n",
      "Transaction ID    0\n",
      "Amount            0\n",
      "Type              0\n",
      "Is Fraud          0\n",
      "dtype: int64\n",
      "\n",
      "Performance Metrics:\n",
      "Precision: 0.0333\n",
      "Recall: 0.0714\n",
      "F1-Score: 0.0455\n",
      "\n",
      "Confusion Matrix:\n",
      "[[257  29]\n",
      " [ 13   1]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Generate Fraud Detection Dataset (fraud_detection.csv)\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of rows in the dataset\n",
    "n = 1000  # You can adjust this value to change the dataset size\n",
    "\n",
    "# Generate random data for each column\n",
    "transaction_ids = np.arange(1001, 1001 + n)  # Transaction ID from 1001 to n+1000\n",
    "amounts = np.random.uniform(10, 1000, n)  # Random transaction amounts between 10 and 1000\n",
    "types = np.random.choice(['credit', 'debit'], n)  # Random transaction type: credit or debit\n",
    "is_fraud = np.random.choice([0, 1], n, p=[0.95, 0.05])  # 5% fraud, 95% legitimate\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Transaction ID': transaction_ids,\n",
    "    'Amount': amounts,\n",
    "    'Type': types,\n",
    "    'Is Fraud': is_fraud\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('fraud_detection.csv', index=False)\n",
    "\n",
    "# Output the first few rows to verify the dataset\n",
    "print(\"Dataset generated and saved as 'fraud_detection.csv'\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Load & Preprocess the Dataset\n",
    "df = pd.read_csv('fraud_detection.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values (if any) - For simplicity, let's fill with mean for numeric columns\n",
    "df['Amount'] = df['Amount'].fillna(df['Amount'].mean())\n",
    "\n",
    "# Step 3: Convert categorical variables using Label Encoding\n",
    "# Convert 'Type' column (e.g., credit/debit) to numeric values using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Type'] = le.fit_transform(df['Type'])\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "# Create new features like transaction-to-amount ratio\n",
    "df['Transaction_to_Amount_Ratio'] = df['Transaction ID'] / df['Amount']\n",
    "\n",
    "# Step 5: Split the dataset into features (X) and target (y)\n",
    "X = df.drop(columns=['Transaction ID', 'Is Fraud'])  # Drop unnecessary columns\n",
    "y = df['Is Fraud']  # Target column: Is Fraud\n",
    "\n",
    "# Step 6: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 7: Train a Decision Tree Classifier\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Evaluate Model Performance\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print confusion matrix for better understanding\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Output performance metrics\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1162e6f-7de5-4134-a375-42de739120a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
